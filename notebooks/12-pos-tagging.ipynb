{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premissa\n",
    "\n",
    "Consideramos `um dataset com tags` e aplicamos `aprendizagem supervisionada`\n",
    "\n",
    "### Fato\n",
    "\n",
    "Essa tarefa envolve o aprendizado de `múltiplas classes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/geanderson/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/geanderson/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import nltk os corpus que vamos utilizar\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "\n",
    "# import das bibliotecas importantes\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown as cb\n",
    "from nltk.corpus import treebank as tb\n",
    "import pprint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: The Fulton County Grand Jury said Friday an...>\n"
     ]
    }
   ],
   "source": [
    "# vamos usar o brown corpus\n",
    "raw_text = nltk.Text(cb.words('ca01'))\n",
    "print (raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n"
     ]
    }
   ],
   "source": [
    "print (cb.words()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "print (cb.tagged_words()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'),\n",
      " ('Fulton', 'NP-TL'),\n",
      " ('County', 'NN-TL'),\n",
      " ('Grand', 'JJ-TL'),\n",
      " ('Jury', 'NN-TL'),\n",
      " ('said', 'VBD'),\n",
      " ('Friday', 'NR'),\n",
      " ('an', 'AT'),\n",
      " ('investigation', 'NN'),\n",
      " ('of', 'IN'),\n",
      " (\"Atlanta's\", 'NP$'),\n",
      " ('recent', 'JJ'),\n",
      " ('primary', 'NN'),\n",
      " ('election', 'NN'),\n",
      " ('produced', 'VBD'),\n",
      " ('``', '``'),\n",
      " ('no', 'AT'),\n",
      " ('evidence', 'NN'),\n",
      " (\"''\", \"''\"),\n",
      " ('that', 'CS'),\n",
      " ('any', 'DTI'),\n",
      " ('irregularities', 'NNS'),\n",
      " ('took', 'VBD'),\n",
      " ('place', 'NN'),\n",
      " ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences_brown_corpus = nltk.corpus.brown.tagged_sents()\n",
    "pprint.pprint(tagged_sentences_brown_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penn-Treebank Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Pierre Vinken , 61 years old , will...>\n"
     ]
    }
   ],
   "source": [
    "raw_text = nltk.Text(tb.words()[0:10])\n",
    "print (raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the']\n"
     ]
    }
   ],
   "source": [
    "print (tb.words()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'),\n",
      " ('Vinken', 'NNP'),\n",
      " (',', ','),\n",
      " ('61', 'CD'),\n",
      " ('years', 'NNS'),\n",
      " ('old', 'JJ'),\n",
      " (',', ','),\n",
      " ('will', 'MD'),\n",
      " ('join', 'VB'),\n",
      " ('the', 'DT'),\n",
      " ('board', 'NN'),\n",
      " ('as', 'IN'),\n",
      " ('a', 'DT'),\n",
      " ('nonexecutive', 'JJ'),\n",
      " ('director', 'NN'),\n",
      " ('Nov.', 'NNP'),\n",
      " ('29', 'CD'),\n",
      " ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences_treebank_corpus = nltk.corpus.treebank.tagged_sents()\n",
    "pprint.pprint (tagged_sentences_treebank_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentenças com Tags:  3914\n"
     ]
    }
   ],
   "source": [
    "print (\"Sentenças com Tags: \", len(tagged_sentences_treebank_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras com Tags: 100676\n"
     ]
    }
   ],
   "source": [
    "print (\"Palavras com Tags:\", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gerando as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    return {\n",
    "    'word': sentence[index],\n",
    "    'is_first': index == 0,\n",
    "    'is_last': index == len(sentence) - 1,\n",
    "    'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "    'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "    'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "    'prefix-1': sentence[index][0],\n",
    "    'prefix-2': sentence[index][:2],\n",
    "    'prefix-3': sentence[index][:3],\n",
    "    'suffix-1': sentence[index][-1],\n",
    "    'suffix-2': sentence[index][-2:],\n",
    "    'suffix-3': sentence[index][-3:],\n",
    "    'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "    'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "    'has_hyphen': '-' in sentence[index],\n",
    "    'is_numeric': sentence[index].isdigit(),\n",
    "    'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capitals_inside': False,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': False,\n",
      " 'is_all_lower': False,\n",
      " 'is_capitalized': True,\n",
      " 'is_first': True,\n",
      " 'is_last': False,\n",
      " 'is_numeric': False,\n",
      " 'next_word': 'is',\n",
      " 'prefix-1': 'T',\n",
      " 'prefix-2': 'Th',\n",
      " 'prefix-3': 'Thi',\n",
      " 'prev_word': '',\n",
      " 'suffix-1': 's',\n",
      " 'suffix-2': 'is',\n",
      " 'suffix-3': 'his',\n",
      " 'word': 'This'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformando o dataset\n",
    "\n",
    "Extraindo palavras de frases marcadas usando a função `untag`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforformar o dataset em X, Y pares onde X = features e Y = POS labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construir o training e testing dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(.80 * len(tagged_sentences_treebank_corpus))\n",
    "training_sentences = tagged_sentences_treebank_corpus[:cutoff]\n",
    "test_sentences = tagged_sentences_treebank_corpus[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131\n",
      "783\n"
     ]
    }
   ],
   "source": [
    "print (len(training_sentences))\n",
    "print (len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = transform_to_dataset(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80637\n",
      "80637\n"
     ]
    }
   ],
   "source": [
    "print(len(X)) \n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Treinando o Modelo de Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('classifier', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use apenas as primeiras 10 mil amostras se estiver executando várias vezes\n",
    "clf.fit(X[:20000],y[:20000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mensurando a Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:91.606%\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy:{:.3%}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar POS tags para sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence):\n",
    "    tagged_sentence = []\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This            DT\n",
      "is              VBZ\n",
      "my              PRP$\n",
      "friend          NN\n",
      ",                ,\n",
      "John            NNP\n",
      ".                .\n"
     ]
    }
   ],
   "source": [
    "POS_list = list(pos_tag(word_tokenize('This is my friend, John.')))\n",
    "for t in POS_list:\n",
    "    print(u\"{:<16}{:>2}\".format(str(t[0]),str(t[1])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We              IN\n",
      "will            MD\n",
      "meet            VB\n",
      "at              IN\n",
      "eight           NN\n",
      "o'clock         NN\n",
      "on              IN\n",
      "Thursday        NNP\n",
      "morning         NN\n",
      ".                .\n"
     ]
    }
   ],
   "source": [
    "POS_list = list(pos_tag(word_tokenize(\"We will meet at eight o'clock on Thursday morning.\")))\n",
    "for t in POS_list:\n",
    "    print(u\"{:<16}{:>2}\".format(str(t[0]),str(t[1])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander       JJR\n",
      ",                ,\n",
      "the             DT\n",
      "great           NN\n",
      "...              :\n",
      "!               CD\n"
     ]
    }
   ],
   "source": [
    "POS_list = list(pos_tag(word_tokenize('Alexander, the great...!')))\n",
    "for t in POS_list:\n",
    "    print(u\"{:<16}{:>2}\".format(str(t[0]),str(t[1])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander       IN\n",
      "the             DT\n",
      "Great           NNP\n",
      ",                ,\n",
      "was             VBD\n",
      "a               DT\n",
      "king            NN\n",
      "of              IN\n",
      "the             DT\n",
      "ancient         DT\n",
      "Greek           NNP\n",
      "kingdom         NN\n",
      "of              IN\n",
      "Macedon         NNP\n",
      ".                .\n"
     ]
    }
   ],
   "source": [
    "POS_list = list(pos_tag(word_tokenize('Alexander the Great, was a king of the ancient Greek kingdom of Macedon.')))\n",
    "for t in POS_list:\n",
    "    print(u\"{:<16}{:>2}\".format(str(t[0]),str(t[1])))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fim!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
